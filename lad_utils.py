# general utilities supporting analysis of data for LAD atlas
# Kathleen Keough 2020, Pollard Lab

from pomegranate import *
import pandas as pd
import numpy as np
from pybedtools import BedTool
import warnings
import json
from functools import reduce
warnings.simplefilter(action='ignore', category=FutureWarning)
import os
import scipy.stats as stats

def bigwigs_to_bedgraphs(bigwigs_dir, bedgraphs_dir, cmds_out):
    # this just makes a bunch of bigwig --> bedgraph 
    # conversion commands to I can run them in parallel
    # cmds_out should end in .txt
    
    commands = []

    for f in os.listdir(f'{bigwigs_dir}'):
        if f.endswith('bigwig'):
            fname = f.split('.')[0]
            commands.append(f'bigWigToBedGraph {bigwigs_dir}/{f} {bedgraphs_dir}/{fname}.bedgraph')
    
    with open(f'{cmds_out}','w') as f:
        for cmd in bedgraph_cmds:
            f.write(cmd + '\n')
        
def parse_bedgraph(f, binsize, counter, blacklistbed, umapbed):
    # parses bedgraph file

    canonical_chroms = [ 'chr' + str(x) for x in range(1,23) ] + ['chrY', 'chrX']
    
    indf_pre = pd.read_csv(f, sep='\t', header=None, 
                       names = ['chrom',
                                'start',
                                'stop',
                                f'score'], low_memory=False).query('chrom in @canonical_chroms')
    
    inbed = BedTool.from_dataframe(indf_pre).sort()
    inbed_filt = inbed.subtract(blacklistbed).sort().intersect(umap_bed).sort()
    
    indf = inbed_filt.to_dataframe()
    indf.columns = ['chrom','start','stop',f'score']

    indf['size'] = indf['stop'] - indf['start']

    to_add = []

    indf_split = indf.query('size > @binsize').copy()
    for ix, row in indf_split.iterrows():
        n_intervals = row['size'] / binsize
        start = row['start']
        chrom = row['chrom']
        score = row[f'score']
        for val in range(int(n_intervals)):
            to_add.append(pd.DataFrame({
                'chrom':[row['chrom']],
                'start':[start],
                'stop':[start + binsize],
                # f'score{counter}':[score]
                'score':[score]
            }))
            start = start + binsize

    indf = pd.concat([indf.query('size == @binsize')[['chrom','start','stop',f'score']],
                      pd.concat(to_add)]).query('chrom in @canonical_chroms')
    return(f'score', indf)

def hmm_predict(cell_type, trained_model, bigwig_data, binsize, print_status_updates=True):
    
    """
    HMM must be pretrained (train_HMM.py) and saved as json
    cell_type: the cell type you're predicting on
    trained_model: json file containing trained HMM
    bigwig_data: path to directory containing bedGraph files named "CellType.bedgraph"

    Returns pandas dataframe with state assignments per bin 
    """
    
    if print_status_updates:
        print(cell_type)

    if type(trained_model) == str:
        with open(trained_model) as json_file:
            model = HiddenMarkovModel.from_json(json.load(json_file))
    else:
        model = trained_model
    
    # put bedgraph filenames in list
    
    fs = []
    
    all_fs = os.listdir(bigwig_data)
    
    for file in all_fs:
        if file.startswith(cell_type) and file.endswith('bedgraph'):
            fs.append(os.path.join(bigwig_data, file))

    # save bedgraph data to dataframes
    
    counter = -1
    cols = []
    
    dfs = []
    
    for f in fs:
        counter += 1
        col, indf = parse_bedgraph(f, binsize, counter, encode_blacklist, umap_bed)
        cols.append(f'score{counter}')
        dfs.append(indf)
        
#     merge_df_out = reduce(lambda  left,right: pd.merge(left,right,on=['chrom','start','stop'],
#                                     how='outer'), dfs)
    
    merge_df_out = dfs[0].merge(dfs[1],on=['chrom','start','stop'],
                                    how='inner')
    merge_df_out = merge_df_out.dropna().drop_duplicates().reset_index(drop=True)
    
    # make predictions on these sequences
    
    merge_df_out['hmm_pred'] = model.predict(merge_df_out[cols].to_numpy(), algorithm = 'viterbi')[1:]
    
    if print_status_updates:
        n_states = len(merge_df_out['hmm_pred'].unique())
        print(f'{n_states} components identified')

    return(merge_df_out)

def assign_categories(dat, cat_names):
    """
    Assign categories to specific HMM states
    based on the mean ChIP-seq coverage
    in bins assigned to that state.
    
    dat is the binned data with HMM predicted states
    generated by lad_utils.hmm_predict()
    
    cat_names should be provided based on which should be
    assigned to sequecing coverage level in ascending order.
    """
    n_states = len(dat['hmm_pred'].unique())
    n_cats = len(cat_names)
    if n_states != n_cats:
        print(f'{n_states} in data, but {n_cats} provided. '\
              f'Number of categories must match the number '\
              f'of HMM states.')
    else:
        
        cols = ['score0','score1'] # each cell type has 2 replicates
        # cols = ['score']
        
        states = []
        means = []
        
        for state in dat['hmm_pred'].unique():
            states.append(state)
            means.append(np.mean(dat.query(f'hmm_pred == {state}')[cols].mean()))
           
        # sort means and associated states, ascending
        sorted_means = sorted(means)
        means_to_states = dict(zip(means, states))
        
        sorted_states = []
        
        for val in sorted_means:
            sorted_states.append(means_to_states[val])
    
        names_to_states = dict(zip(sorted_states, cat_names))
        dat['category'] = dat['hmm_pred'].replace(names_to_states)
        return dat

def assign_categories_sep(dat, cat_names):
    """
    Assign categories to specific HMM states
    based on the mean ChIP-seq coverage
    in bins assigned to that state.
    
    dat is the binned data with HMM predicted states
    generated by lad_utils.hmm_predict()
    
    cat_names should be provided based on which should be
    assigned to sequecing coverage level in ascending order.
    """
    n_states = len(dat['hmm_pred'].unique())
    n_cats = len(cat_names)
    if n_states != n_cats:
        print(f'{n_states} in data, but {n_cats} provided. '\
              f'Number of categories must match the number '\
              f'of HMM states.')
    else:
        
        # cols = ['score0','score1'] # each cell type has 2 replicates
        cols = ['score']
        
        states = []
        means = []
        
        for state in dat['hmm_pred'].unique():
            states.append(state)
            means.append(np.mean(dat.query(f'hmm_pred == {state}')[cols].mean()))
           
        # sort means and associated states, ascending
        sorted_means = sorted(means)
        means_to_states = dict(zip(means, states))
        
        sorted_states = []
        
        for val in sorted_means:
            sorted_states.append(means_to_states[val])
    
        names_to_states = dict(zip(sorted_states, cat_names))
        dat['category'] = dat['hmm_pred'].replace(names_to_states)
        return dat

def AIC(row):
    L = row['log_probs']
    n_states = row['n_states']
    n_params = 2
    
    return(-2*(L) + 2*(n_states**2 + n_params*n_states - 1))

def BIC(row):
    n_states = row['n_states']
    L = row['log_probs']
    p = n_states**2 + 2*n_states - 1
    
    return(-2*(L) + p*np.log(3088252892))

def save_dat_to_bed(dat, names, outdir, filterfile):
    """
    Merge adjacent bins with same category to get BED file of domains
    
    dat: output from HMM
    names: names of categories of element, in ascending order of expected ChIP read depth
    outdir: where to save the BED files
    filterfile: pybedtools BedTool, use to filter things you don't want included in the final set, e.g. ENCODE blacklist
    """
    dat_w_cats = assign_categories(dat, names)
    dat_w_cats['id'] = dat_w_cats.index
    
    # make directory if it doesn't exist
    if not os.path.exists(outdir):
        os.makedirs(outdir)
    
    for cat in dat_w_cats['category'].unique():
        for ct in dat_w_cats['cell_type'].unique():
            dat_bed = BedTool.from_dataframe(dat_w_cats.query('(cell_type == @ct) and (category == @cat)')[['chrom','start','stop','id']]).sort().merge().subtract(filterfile).to_dataframe()
            dat_bed.to_csv(os.path.join(outdir, f'{ct}_{cat}.bed'),
                                       sep='\t', index=False, header=False)

def save_dat_to_bed_from_file(dat_dir, names, outdir):
    """
    Merge adjacent bins with same category to get BED file of domains
    
    dat: output from HMM
    names: names of categories of element, in ascending order of expected ChIP read depth
    outdir: where to save the BED files
    filterfile: pybedtools BedTool, use to filter things you don't want included in the final set, e.g. ENCODE blacklist
    """
    
    # make directory if it doesn't exist
    if not os.path.exists(outdir):
        os.makedirs(outdir)
    
    for f in os.listdir(dat_dir):
        ct = f.split('.')[0]
        dat = pd.read_table(os.path.join(dat_dir, f))
        dat_w_cats = assign_categories(dat, names)
        dat_w_cats['id'] = dat_w_cats.index
        for cat in dat_w_cats['category'].unique():
            dat_bed = BedTool.from_dataframe(dat_w_cats.query('(category == @cat)')[['chrom','start','stop','id']]).sort().merge().to_dataframe()
            dat_bed.to_csv(os.path.join(outdir, f'{ct}_{cat}.bed'),
                                       sep='\t', index=False, header=False)
            
def get_enrichment(feature, dat):
    ors = []
    pvals = []
    cats = []

    for cat in dat['category'].unique():

        counts = np.array([[len(dat.query(f'{feature} and category == @cat')),
                  len(dat.query(f'~{feature} and category == @cat'))],
                 [len(dat.query(f'{feature} and category != @cat')),
                  len(dat.query(f'~{feature} and category != @cat'))]])

        odds, pval = stats.fisher_exact(counts)
        ors.append(odds)
        pvals.append(pval)
        cats.append(cat)

    out = pd.DataFrame({
        'odds_ratio':ors,
        'fishers_p':pvals,
        'cat':cats
    })
    
    return(out)           


lad_replace = {'LAD':'T1-LAD',
              'LADlite':'T2-LAD'}

ct_replace = {
    'CardiacMyocytes':'Cardiac myocytes',
    'EarlySomite':'Early somite',
    'H9ESC':'ESCs',
    'ParaxMesoderm':'Paraxial mesoderm',
    'DefEctoderm':'Definitive ectoderm',
    'MidHindgut':'Mid-hindgut',
    'D5Midbrain':'Midbrain',
    'Liver':'Liver progenitors',
    'SortedCM':'Sorted cardiac myocytes',
    'BorderEctoderm':'Border ectoderm',
    'EndoProgenitor':'Endothelial progenitors',
    'D4Artery':'Artery progenitors'
}

kdd_replace = {
    'HAD':'KDD',
    'nonHAD':'non-KDD'
}

cell_types_f = 'cell_types.txt'
with open(cell_types_f, 'r') as f:
    all_cts = f.read().splitlines()

encode_blacklist = BedTool('encode_hg38_blacklist.bed')
umap_bed = BedTool('umap_s100_hg38.bed')
